input {
	kafka {
		bootstrap_servers => "${kafka_hosts}" #这里可以是kafka集群，如"192.168.149.101:9092,192.168.149.102:9092,192.168.149.103:9092"
        auto_offset_reset => "earliest"
       # auto_offset_reset => "latest"
	    topics => ["filebeat"]
		#consumer_threads => 4 #分区数
    }
}

filter {
	json {
		source => "message"
	}

	grok {
		match => {"message" => "%{TIMESTAMP_ISO8601:log_time} %{LOGLEVEL:level} \[(?<process>.*)\] (?<component>[a-zA-Z0-9\.]+): (?<content>.*)"}
		remove_field => ["message"]
	}

	date {
		match => ["log_time", "ISO8601"]
		remove_field => ["log_time"]
	}
}
## Add your filters / logstash plugins configuration here

output {
	#elasticsearch {
	#	hosts => "elasticsearch:9200"
	#	user => "elastic"
	#	password => "changeme"
	#	index => "app-%{+YYYY.MM.dd}"
	#}
	#file {
	#	path => "output/Hadoop.log"
	#}
	kafka {
		bootstrap_servers => "${kafka_hosts}" #这里可以是kafka集群，如"192.168.149.101:9092,192.168.149.102:9092,192.168.149.103:9092"
	    topic_id => "%{app}_log_input"
		codec => json
		compression_type => "lz4"
		linger_ms => 50
		partitioner => "round_robin"
	}
	#stdout {}
}
