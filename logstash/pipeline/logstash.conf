input {
	kafka {
		bootstrap_servers => "${kafka_hosts}" #这里可以是kafka集群，如"192.168.149.101:9092,192.168.149.102:9092,192.168.149.103:9092"
        auto_offset_reset => "earliest" # offset设为最早的
       # auto_offset_reset => "latest" # offset设为最近一个的
	    topics => ["filebeat"] # 指定读取的topic
		#consumer_threads => 4 # 设置为kafka分区数
    }
}

filter {
	json {
		source => "message" # 消息从kafka传到logstash时，会被包装在message字段中；此步将message中的字段提取到最外层
	}

	# 如果app是hadoop
	if [app] == "hadoop" {
		grok {
		# 语法见grok语法与logstash官方文档中的过滤器插件-grok部分，将文本形式的message解析为结构化json，将这些字段添加到数据中
		match => {"message" => "%{TIMESTAMP_ISO8601:log_time} %{LOGLEVEL:level} \[(?<process>.*)\] (?<component>[a-zA-Z0-9\.]+): (?<content>.*)"}
		remove_field => ["message"] # 解析完后message字段没有用了，删除
		}

		# 格式化时间
		date {
		match => ["log_time", "ISO8601"] # 识别ISO8601格式的log_time字段，默认将格式化后的时间放到@timestamp中
		remove_field => ["log_time"] # 没有用了 删除
		}
	} 
}
## Add your filters / logstash plugins configuration here

output {
	elasticsearch {
		hosts => "elasticsearch:9200"
		user => "elastic"
		password => "changeme"
		index => "%{app}-%{+YYYY.MM.dd}" # 索引格式 %{}会将{}内的内容替换为数据中对应字段的值
	}
	kafka {
		bootstrap_servers => "${kafka_hosts}" #这里可以是kafka集群，如"192.168.149.101:9092,192.168.149.102:9092,192.168.149.103:9092"
	    topic_id => "%{app}_log_input"
		codec => json # json编码
		compression_type => "lz4" # 压缩算法
		linger_ms => 50 # 等待50ms在发送，批处理
		partitioner => "round_robin" # 负载均衡策略
	}
	#stdout {}
}
